{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DDQN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPnp+nrL/Ab0ApBwBFvEora"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"uAac41OTp3tV","colab_type":"code","colab":{}},"source":["import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","\n","cuda_enabled = T.cuda.is_available()\n","print('cuda enabled? : ', cuda_enabled)\n","\n","saved_model_path = 'saved_models/trained_model_'\n","\n","class DoubleDQN(nn.Module):\n","    def __init__(self, lr, input_dims, n_actions):\n","        super(DoubleDQN, self).__init__()\n","\n","        self.input_dims = input_dims\n","        self.n_actions = n_actions\n","\n","        #set up network model architecture\n","        self.fc1 = nn.Linear(*self.input_dims, 256)\n","        self.fc2 = nn.Linear(256, 512)\n","        self.fc3 = nn.Linear(512,512)\n","        self.fc4 = nn.Linear(512, self.n_actions)\n","        #Linear model. 1 input layer, 2 hidden layers, 1 output layer w/ n_actions outputs\n","\n","        #state optimizer and loss func / cost criterion\n","        self.optimizer = optim.Adam(self.parameters(), lr = lr)\n","        self.loss = nn.MSELoss()\n","        #if cuda capabilities are enabled, set device to cuda -- else cpu\n","        if cuda_enabled:#set default tensor data type if we have cuda capabilities\n","            T.set_default_tensor_type('torch.cuda.FloatTensor')#use gpu\n","        self.device = T.device('cuda:0' if (cuda_enabled) else 'cpu')#ternary\n","        #cast to device\n","        self.to(self.device)\n","\n","    def forward(self, state):\n","        \"\"\"\n","            Given a state, pass sequentially through the network and return\n","            the action space\n","        \"\"\"\n","        x = F.relu(self.fc1(state))#use ReLu activation func for all layers except last (output layer)\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        actions = self.fc4(x)\n","        return actions\n","    def save_model(self, type):\n","        print('...saving model...')\n","        path = saved_model_path + type + '.pt'\n","        T.save(self.state_dict(), path)\n","    def load_model(self, type):\n","        print('...loading model...')\n","        path = saved_model_path + type + '.pt'\n","        self.load_state_dict(T.load(path))\n","\n","class Agent:\n","    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n","                    max_mem_size = 50000, eps_end = 0.01, eps_dec = 5e-4):\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.eps_min = eps_end\n","        self.eps_dec = eps_dec\n","        self.lr = lr\n","        self.action_space = [i for i in range(n_actions)]#list comprehension\n","        self.mem_size = max_mem_size\n","        self.batch_size = batch_size\n","        self.mem_cntr = 0\n","        self.iter_cntr = 0#keep track of how many iterations have been run (for replace)\n","        self.replace_target = 100#how many iterations till we replace our target network\n","\n","        #local network is the one we are training\n","        self.Q_local = DoubleDQN(lr,n_actions = n_actions, input_dims = input_dims)\n","        #target network is what we use to determine the best action available (for bellman eq.)\n","        self.Q_target = DoubleDQN(lr,n_actions = n_actions, input_dims = input_dims)\n","\n","        #setup numpy zeros-arrays with max mem size\n","        sz = self.mem_size\n","        dims = input_dims\n","        self.state_memory = np.zeros((sz,*dims), dtype = np.float32)\n","        self.new_state_memory = np.zeros((sz,*dims), dtype = np.float32)\n","        self.action_memory = np.zeros(sz, dtype = np.int64)#action's are ints (one-hot encoded??!)\n","        self.reward_memory = np.zeros(sz, dtype = np.float32)\n","        self.terminal_memory = np.zeros(sz, dtype = np.bool)#done flags\n","\n","    def store_transition(self, state, action, reward, state_, terminal):\n","        \"\"\"\n","            store a new memory.\n","            state is the current environment observation\n","            action is the action we performed\n","            reward is the reward we got\n","            state_ is the next state after our action/reward feedback\n","            terminal is a done flag if we finished the epsiode (99% sure ???)\n","        \"\"\"\n","        index = self.mem_cntr % self.mem_size#handles wrap-around. We rewrite old memories after mem counter exceeds max_mem_size\n","        self.state_memory[index] = state\n","        self.new_state_memory[index] = state_\n","        self.reward_memory[index] = reward\n","        self.action_memory[index] = action\n","        self.terminal_memory[index] = terminal\n","        #we added a memory so increase memory counter\n","        self.mem_cntr += 1\n","\n","    def choose_action(self, observation):\n","        #epsilon-greedy algorithm\n","        if np.random.random() > self.epsilon:#exploit\n","            #convert observation to a state tensor and cast to device\n","            state = T.tensor([observation], dtype= T.float).to(self.Q_local.device)\n","            #evaluates Q-Values for each action given a state\n","            actions = self.Q_local.forward(state)\n","            #argmax() chooses best action (highest Q-Value)\n","            action = T.argmax(actions).item()#.item() to get index\n","        else:#explore\n","            action = np.random.choice(self.action_space)#random action in Agent's action space\n","        #return our chosen action (either exploited or explored )\n","        return action\n","\n","    def learn(self):\n","        if self.mem_cntr < self.batch_size:#if we can't complete a batch, then dont learn\n","            return\n","        #zero out our gradient. (stops exploding gradient because torch remembers past gradient pos ??? I think...)\n","        self.Q_local.optimizer.zero_grad()\n","        #replace target network every replace_target iterations\n","        self.replace_target_network()\n","        max_mem = min(self.mem_cntr, self.mem_size)#since mem_cntr can be larger than our saved memory due to wrap-around hack, we must take the smaller one\n","\n","        #define our batch stochastically\n","        batch = np.random.choice(max_mem, self.batch_size, replace = False)\n","        #random.choice returns batch_size amount of values in the range of max_mem. randomly.\n","        #generate our batch_index using np.arange ex: b_i = [0,1,2,3,4,batch_size - 1]\n","        batch_index = np.arange(self.batch_size)\n","\n","        #define our batches with each given metric (rule of 5)\n","        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_local.device)\n","        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_local.device)\n","        action_batch = T.tensor(self.action_memory[batch]).to(self.Q_local.device)\n","        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_local.device)\n","        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_local.device)\n","\n","        #forward pass of local and target networks\n","        q_pred = self.Q_local.forward(state_batch)[batch_index, action_batch]\n","        q_next = self.Q_target.forward(new_state_batch)\n","        q_eval = self.Q_local.forward(new_state_batch)#no idea what this is for\n","        #i think q_eval is for finding the best predicted action for the new_state_batch but idk why we want it\n","        max_actions = T.argmax(q_eval, dim = 1)\n","        q_next[terminal_batch] = 0.0\n","        # estimated future reward is the self.gamma*q_next[batch_index, max_actions] part, where gamma discounts future rewards\n","        q_target = reward_batch + self.gamma*q_next[batch_index, max_actions]#bellman equation\n","        loss = self.Q_local.loss(q_target, q_pred).to(self.Q_local.device)#basically put, how good could our action have been(q-target), vs how good it actually was (q-pred)\n","        loss.backward()\n","\n","        self.Q_local.optimizer.step()\n","        self.iter_cntr += 1\n","        self.decrement_epsilon()\n","\n","    def decrement_epsilon(self):\n","        self.epsilon = max((self.epsilon - self.eps_dec), self.eps_min)\n","\n","    def replace_target_network(self):\n","        if self.iter_cntr is not None and self.iter_cntr % self.replace_target == 0:\n","            self.Q_target.load_state_dict(self.Q_local.state_dict())\n","\n","    def save_agent(self):\n","        #save local and target network\n","        self.Q_local.save_model(\"local\")\n","        self.Q_target.save_model(\"target\")\n","\n","    def load_agent(self):\n","        #load local and target network\n","        self.Q_local.load_model(\"local\")\n","        self.Q_target.load_model(\"target\")"],"execution_count":0,"outputs":[]}]}