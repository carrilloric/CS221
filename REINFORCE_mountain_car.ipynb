{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"REINFORCE_mountain_car.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPu0ST8wlzoj5Yufh/K6wws"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"uCH6tdo3mntw","colab_type":"code","outputId":"277204f8-6d0d-4f75-92a7-e2a00aa94201","executionInfo":{"status":"error","timestamp":1591278887311,"user_tz":240,"elapsed":864,"user":{"displayName":"ricardo carrillo","photoUrl":"","userId":"05727590381692514706"}},"colab":{"base_uri":"https://localhost:8080/","height":367}},"source":[""],"execution_count":0,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-881d294f6ba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#from rl.reinforce.agent import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# MountainCarContinuous-v0 defines \"solving\" as getting an average reward of 90.0 over 100 consecutive trials.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'agent'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"VnCG4gAumE3E","colab_type":"code","outputId":"473b7696-d83e-4db4-fdd5-f4a8ceec19f6","executionInfo":{"status":"ok","timestamp":1591495798630,"user_tz":240,"elapsed":9472,"user":{"displayName":"ricardo carrillo","photoUrl":"","userId":"05727590381692514706"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Ricardo Carrillo REINFORCE MOUNTAIN_CAR\n","%tensorflow_version 1.x\n","import numpy as np\n","import sklearn\n","import sklearn.preprocessing\n","import tensorflow as tf\n","from tensorflow.contrib import rnn\n","\n","\n","class MLPStochasticPolicyAgent:\n","    def __init__(self, env, num_input, init_learning_rate=5e-6, min_learning_rate=1e-9, learning_rate_N_max=2000,\n","                 shuffle=True, batch_size=1, sigma=None):\n","        self._env = env\n","        self._sess = tf.Session()\n","        self._states = tf.placeholder(tf.float32, (None, num_input), name=\"states\")\n","\n","        self._init_learning_rate = init_learning_rate\n","        self._min_learning_rate = min_learning_rate\n","        self._learning_rate_N_max = learning_rate_N_max\n","        self._learning_rate = tf.placeholder(tf.float32, shape=[])\n","\n","        self._phi_hidden = 128\n","        self._sigma_hidden = 32\n","\n","        # policy parameters\n","        self._mu_theta = tf.get_variable(\"mu_theta\", [self._phi_hidden, 1],\n","                                         initializer=tf.zeros_initializer())\n","        if sigma is None:\n","            self._sigma_theta = tf.get_variable(\"sigma_theta\", [self._sigma_hidden],\n","                                                initializer=tf.zeros_initializer())\n","\n","        # neural featurizer parameters\n","        self._W1 = tf.get_variable(\"W1\", [num_input, self._phi_hidden],\n","                                   initializer=tf.random_normal_initializer())\n","        self._b1 = tf.get_variable(\"b1\", [self._phi_hidden],\n","                                   initializer=tf.constant_initializer(0))\n","        self._h1 = tf.nn.tanh(tf.matmul(self._states, self._W1) + self._b1)\n","        self._W2 = tf.get_variable(\"W2\", [self._phi_hidden, self._phi_hidden],\n","                                   initializer=tf.random_normal_initializer())\n","        self._b2 = tf.get_variable(\"b2\", [self._phi_hidden],\n","                                   initializer=tf.constant_initializer(0))\n","        self._phi = tf.nn.tanh(tf.matmul(self._h1, self._W2) + self._b2)\n","\n","        self._mu = tf.matmul(self._phi, self._mu_theta)\n","        if sigma is None:\n","            self._sigma = tf.reduce_sum(self._sigma_theta)\n","            self._sigma = tf.exp(self._sigma)\n","        else:\n","            self._sigma = tf.constant(sigma, dtype=tf.float32)\n","\n","        self._optimizer = tf.train.GradientDescentOptimizer(learning_rate=self._learning_rate)\n","\n","        self._discounted_rewards = tf.placeholder(tf.float32, (None, 1), name=\"discounted_rewards\")\n","        self._taken_actions = tf.placeholder(tf.float32, (None, 1), name=\"taken_actions\")\n","\n","        # we'll get the policy gradient by using -log(pdf), where pdf is the PDF of the Normal distribution\n","        self._loss = -tf.log(tf.sqrt(1/(2 * np.pi * self._sigma**2)) * tf.exp(-(self._taken_actions - self._mu)**2/(2 * self._sigma**2))) * self._discounted_rewards\n","\n","        self._train_op = self._optimizer.minimize(self._loss)\n","\n","        self._sess.run(tf.global_variables_initializer())\n","\n","        self._num_input = num_input\n","        self._shuffle = shuffle\n","        self._batch_size = batch_size\n","        # rollout buffer\n","        self._state_buffer  = []\n","        self._reward_buffer = []\n","        self._action_buffer = []\n","        # record reward history for normalization\n","        self._all_rewards = []\n","        self._max_reward_length = 1000000\n","        self._discount_factor = 0.99\n","\n","        observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n","        self._scaler = sklearn.preprocessing.StandardScaler()\n","        self._scaler.fit(observation_examples)\n","\n","    def sample_action(self, system_state):\n","        system_state = self._scaler.transform(system_state.reshape(1, -1))\n","        # Gaussian policy\n","        mu, sigma = self._sess.run([self._mu, self._sigma], feed_dict={\n","            self._states: system_state\n","        })\n","        action = np.random.normal(mu, sigma)\n","        action = np.clip(action, self._env.action_space.low[0], self._env.action_space.high[0])\n","        return action[0], sigma\n","\n","    def store_rollout(self, state, action, reward):\n","        self._action_buffer.append(action)\n","        self._reward_buffer.append(reward)\n","        self._state_buffer.append(state)\n","\n","    def update_model(self, iteration):\n","        N = len(self._reward_buffer)\n","        r = 0  # use discounted reward to approximate Q value\n","\n","        discounted_rewards = np.zeros(N)\n","        for t in reversed(range(N)):\n","            r = self._reward_buffer[t] + self._discount_factor * r\n","            discounted_rewards[t] = r\n","\n","        # reduce gradient variance by normalization\n","        self._all_rewards += discounted_rewards.tolist()\n","        self._all_rewards = self._all_rewards[:self._max_reward_length]\n","        discounted_rewards -= np.mean(self._all_rewards)\n","        discounted_rewards /= np.std(self._all_rewards)\n","\n","        learning_rate = self._gen_learning_rate(iteration, l_max=self._init_learning_rate,\n","                                                l_min=self._min_learning_rate, N_max=self._learning_rate_N_max)\n","\n","        all_samples = []\n","        for t in range(N-1):\n","            state  = self._state_buffer[t]\n","            action = self._action_buffer[t]\n","            reward = [discounted_rewards[t]]\n","            sample = [state, action, reward]\n","            all_samples.append(sample)\n","        if self._shuffle:\n","            np.random.shuffle(all_samples)\n","\n","        batches = list(self._minibatches(all_samples, batch_size=self._batch_size))\n","\n","        for b in range(len(batches)):\n","            batch = batches[b]\n","            states = [row[0] for row in batch]\n","            actions = [row[1] for row in batch]\n","            rewards = [row[2] for row in batch]\n","\n","            self._sess.run([self._train_op], {\n","                self._states:             states,\n","                self._taken_actions:      actions,\n","                self._discounted_rewards: rewards,\n","                self._learning_rate:      learning_rate\n","            })\n","\n","        self._clean_up()\n","\n","    def _minibatches(self, samples, batch_size):\n","        for i in range(0, len(samples), batch_size):\n","            yield samples[i:i + batch_size]\n","\n","    def _gen_learning_rate(self, iteration, l_max, l_min, N_max):\n","        if iteration > N_max:\n","            return l_min\n","        alpha = 2 * l_max\n","        beta = np.log((alpha / l_min - 1)) / N_max\n","        return alpha / (1 + np.exp(beta * iteration))\n","\n","    def _clean_up(self):\n","        self._state_buffer  = []\n","        self._reward_buffer = []\n","        self._action_buffer = []\n","\n","\n","class TFRecurrentStochasticPolicyAgent:\n","    def __init__(self, env, num_input, init_learning_rate=5e-6, min_learning_rate=1e-9, learning_rate_N_max=2000,\n","                 shuffle=True, batch_size=1, sigma=None):\n","        self._env = env\n","        self._sess = tf.Session()\n","\n","        self._init_learning_rate = init_learning_rate\n","        self._min_learning_rate = min_learning_rate\n","        self._learning_rate_N_max = learning_rate_N_max\n","        self._learning_rate = tf.placeholder(tf.float32, shape=[])\n","\n","        self._rnn_hidden = 10\n","        self._rnn_steps = 5\n","        self._sigma_hidden = 32\n","\n","        self._states = tf.placeholder(tf.float32, (None, self._rnn_steps, num_input), name=\"states\")\n","\n","        # policy parameters\n","        self._mu_theta = tf.get_variable(\"mu_theta\", [self._rnn_hidden, 1],\n","                                         initializer=tf.zeros_initializer())\n","        if sigma is None:\n","            self._sigma_theta = tf.get_variable(\"sigma_theta\", [self._sigma_hidden],\n","                                                initializer=tf.zeros_initializer())\n","\n","        # neural featurizer parameters\n","        # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n","        input_sequence = tf.unstack(self._states, self._rnn_steps, 1)\n","        lstm_cell = rnn.LSTMCell(self._rnn_hidden, forget_bias=1.0, initializer=tf.random_normal_initializer())\n","        outputs, _ = rnn.static_rnn(lstm_cell, input_sequence, dtype=tf.float32)\n","        self._phi = outputs[-1]\n","\n","        self._mu = tf.matmul(self._phi, self._mu_theta)\n","        if sigma is None:\n","            self._sigma = tf.reduce_sum(self._sigma_theta)\n","            self._sigma = tf.exp(self._sigma)\n","        else:\n","            self._sigma = tf.constant(sigma, dtype=tf.float32)\n","\n","        self._optimizer = tf.train.GradientDescentOptimizer(learning_rate=self._learning_rate)\n","\n","        self._discounted_rewards = tf.placeholder(tf.float32, (None, 1), name=\"discounted_rewards\")\n","        self._taken_actions = tf.placeholder(tf.float32, (None, 1), name=\"taken_actions\")\n","\n","        # we'll get the policy gradient by using -log(pdf), where pdf is the PDF of the Normal distribution\n","        self._loss = -tf.log(tf.sqrt(1/(2 * np.pi * self._sigma**2)) * tf.exp(-(self._taken_actions - self._mu)**2/(2 * self._sigma**2))) * self._discounted_rewards\n","\n","        self._train_op = self._optimizer.minimize(self._loss)\n","\n","        self._sess.run(tf.global_variables_initializer())\n","\n","        self._num_input = num_input\n","        self._shuffle = shuffle\n","        self._batch_size = batch_size\n","        # rollout buffer\n","        self._state_buffer  = []\n","        self._reward_buffer = []\n","        self._action_buffer = []\n","        # record reward history for normalization\n","        self._all_rewards = []\n","        self._max_reward_length = 1000000\n","        self._discount_factor = 0.99\n","\n","        self._start_state_value = 0.\n","\n","    def sample_action(self, env_state):\n","        states = self._prepare_states(env_state)\n","        # Gaussian policy\n","        mu, sigma = self._sess.run([self._mu, self._sigma], feed_dict={\n","            self._states: np.reshape(states, (1, self._rnn_steps, self._num_input))\n","        })\n","        action = np.random.normal(mu, sigma)\n","        action = np.clip(action, self._env.action_space.low[0], self._env.action_space.high[0])\n","        return action[0], sigma\n","\n","    def _prepare_states(self, env_state):\n","        curr_state = self._filter_state(env_state)\n","        stack_len = self._rnn_steps - 1\n","        states = [[x[0]] for x in self._state_buffer]\n","        if len(states) is 0:\n","            self._start_state_value = env_state[0]\n","        while len(states) < stack_len:\n","            states.insert(0, [self._start_state_value])\n","        return states[-stack_len:] + [curr_state]\n","\n","    def _filter_state(self, env_state):\n","        return [env_state[0]]\n","\n","    def store_rollout(self, env_state, action, reward):\n","        self._action_buffer.append(action)\n","        self._reward_buffer.append(reward)\n","        self._state_buffer.append(self._filter_state(env_state))\n","\n","    def update_model(self, iteration):\n","        N = len(self._reward_buffer)\n","        r = 0  # use discounted reward to approximate Q value\n","\n","        discounted_rewards = np.zeros(N)\n","        for t in reversed(range(N)):\n","            r = self._reward_buffer[t] + self._discount_factor * r\n","            discounted_rewards[t] = r\n","\n","        # reduce gradient variance by normalization\n","        self._all_rewards += discounted_rewards.tolist()\n","        self._all_rewards = self._all_rewards[:self._max_reward_length]\n","        discounted_rewards -= np.mean(self._all_rewards)\n","        discounted_rewards /= np.std(self._all_rewards)\n","\n","        learning_rate = self._gen_learning_rate(iteration, l_max=self._init_learning_rate,\n","                                                l_min=self._min_learning_rate, N_max=self._learning_rate_N_max)\n","\n","        all_samples = []\n","        for t in range(N-1):\n","            last = t + 1\n","            first = max(0, last - self._rnn_steps)\n","            states = self._state_buffer[first:last]\n","            while len(states) < self._rnn_steps:\n","                states.insert(0, [self._start_state_value])\n","            states = np.reshape(np.array(states), (self._rnn_steps, self._num_input))\n","            action = self._action_buffer[t]\n","            reward = [discounted_rewards[t]]\n","            sample = [states, action, reward]\n","            all_samples.append(sample)\n","        if self._shuffle:\n","            np.random.shuffle(all_samples)\n","\n","        batches = list(self._minibatches(all_samples, batch_size=self._batch_size))\n","\n","        for b in range(len(batches)):\n","            batch = batches[b]\n","            states = [row[0] for row in batch]\n","            actions = [row[1] for row in batch]\n","            rewards = [row[2] for row in batch]\n","\n","            self._sess.run([self._train_op], {\n","                self._states:             states,\n","                self._taken_actions:      actions,\n","                self._discounted_rewards: rewards,\n","                self._learning_rate:      learning_rate\n","            })\n","\n","        self._clean_up()\n","\n","    def _minibatches(self, samples, batch_size):\n","        for i in range(0, len(samples), batch_size):\n","            yield samples[i:i + batch_size]\n","\n","    def _gen_learning_rate(self, iteration, l_max, l_min, N_max):\n","        if iteration > N_max:\n","            return l_min\n","        alpha = 2 * l_max\n","        beta = np.log((alpha / l_min - 1)) / N_max\n","        return alpha / (1 + np.exp(beta * iteration))\n","\n","    def _clean_up(self):\n","        self._state_buffer  = []\n","        self._reward_buffer = []\n","        self._action_buffer = []\n","\n","\n","class TFRecurrentStochasticPolicyAgent2:\n","    def __init__(self, env, num_input, init_learning_rate=5e-6, min_learning_rate=1e-9, learning_rate_N_max=2000,\n","                 shuffle=True, batch_size=1):\n","        self._env = env\n","        self._sess = tf.Session()\n","        self._states = tf.placeholder(tf.float32, (None, 1, num_input), name=\"states\")\n","\n","        self._init_learning_rate = init_learning_rate\n","        self._min_learning_rate = min_learning_rate\n","        self._learning_rate_N_max = learning_rate_N_max\n","        self._learning_rate = tf.placeholder(tf.float32, shape=[])\n","\n","        self._rnn_hidden = 128\n","        self._sigma_hidden = 32\n","\n","        # policy parameters\n","        self._mu_theta = tf.get_variable(\"mu_theta\", [self._rnn_hidden, 1],\n","                                         initializer=tf.zeros_initializer())\n","        self._sigma_theta = tf.get_variable(\"sigma_theta\", [self._sigma_hidden],\n","                                            initializer=tf.zeros_initializer())\n","\n","        # LSTM featurizer\n","        input_sequence = tf.unstack(self._states, 1, 1)\n","        self._lstm_cell = rnn.BasicLSTMCell(self._rnn_hidden, forget_bias=1.0)\n","        self._rnn_state_in = self._lstm_cell.zero_state(1, tf.float32)\n","        self._curr_rnn_state = (np.zeros([1, self._rnn_hidden]), np.zeros([1, self._rnn_hidden]))\n","        outputs, self._rnn_state = rnn.static_rnn(self._lstm_cell, input_sequence, dtype=tf.float32, initial_state=self._rnn_state_in)\n","\n","        self._phi = outputs[-1]\n","\n","        self._mu = tf.matmul(self._phi, self._mu_theta)\n","        self._sigma = tf.reduce_sum(self._sigma_theta)\n","        self._sigma = tf.exp(self._sigma)\n","\n","        self._optimizer = tf.train.GradientDescentOptimizer(learning_rate=self._learning_rate)\n","\n","        self._discounted_rewards = tf.placeholder(tf.float32, (None, 1), name=\"discounted_rewards\")\n","        self._taken_actions = tf.placeholder(tf.float32, (None, 1), name=\"taken_actions\")\n","\n","        # we'll get the policy gradient by using -log(pdf), where pdf is the PDF of the Normal distribution\n","        self._loss = -tf.log(tf.sqrt(1/(2 * np.pi * self._sigma**2)) * tf.exp(-(self._taken_actions - self._mu)**2/(2 * self._sigma**2))) * self._discounted_rewards\n","\n","        self._train_op = self._optimizer.minimize(self._loss)\n","\n","        self._sess.run(tf.global_variables_initializer())\n","\n","        self._num_input = num_input\n","        self._shuffle = shuffle\n","        self._batch_size = batch_size\n","        # rollout buffer\n","        self._state_buffer  = []\n","        self._reward_buffer = []\n","        self._action_buffer = []\n","        # record reward history for normalization\n","        self._all_rewards = []\n","        self._max_reward_length = 1000000\n","        self._discount_factor = 0.99\n","\n","    def sample_action(self, system_state):\n","        state = self._filter_state(system_state)\n","        # Gaussian policy\n","        mu, sigma, s = self._sess.run([self._mu, self._sigma, self._rnn_state], feed_dict={\n","            self._states: np.reshape(state, (1, 1, self._num_input)),\n","            self._rnn_state_in: self._curr_rnn_state\n","        })\n","        self._curr_rnn_state = s\n","        action = np.random.normal(mu, sigma)\n","        action = np.clip(action, self._env.action_space.low[0], self._env.action_space.high[0])\n","        return action[0], sigma\n","\n","    def _filter_state(self, env_state):\n","        return [env_state[0]]\n","\n","    def store_rollout(self, env_state, action, reward):\n","        self._action_buffer.append(action)\n","        self._reward_buffer.append(reward)\n","        self._state_buffer.append(self._filter_state(env_state))\n","\n","    def update_model(self, iteration):\n","        N = len(self._reward_buffer)\n","        r = 0 # use discounted reward to approximate Q value\n","\n","        # compute discounted future rewards\n","        discounted_rewards = np.zeros(N)\n","        for t in reversed(range(N)):\n","            # future discounted reward from now on\n","            r = self._reward_buffer[t] + self._discount_factor * r\n","            discounted_rewards[t] = r\n","\n","        # reduce gradient variance by normalization\n","        self._all_rewards += discounted_rewards.tolist()\n","        self._all_rewards = self._all_rewards[:self._max_reward_length]\n","        discounted_rewards -= np.mean(self._all_rewards)\n","        discounted_rewards /= np.std(self._all_rewards)\n","\n","        learning_rate = self._gen_learning_rate(iteration, l_max=self._init_learning_rate,\n","                                                l_min=self._min_learning_rate, N_max=self._learning_rate_N_max)\n","\n","        all_samples = []\n","        for t in range(N-1):\n","            state  = [self._state_buffer[t]]\n","            action = self._action_buffer[t]\n","            reward = [discounted_rewards[t]]\n","            sample = [state, action, reward]\n","            all_samples.append(sample)\n","        if self._shuffle:\n","            np.random.shuffle(all_samples)\n","\n","        batches = list(self._minibatches(all_samples, batch_size=self._batch_size))\n","\n","        curr_rnn_state = (np.zeros([1, self._rnn_hidden]), np.zeros([1, self._rnn_hidden]))\n","\n","        for b in range(len(batches)):\n","            batch = batches[b]\n","            states = [row[0] for row in batch]\n","            actions = [row[1] for row in batch]\n","            rewards = [row[2] for row in batch]\n","            # perform one update of training\n","            _, s = self._sess.run([self._train_op, self._rnn_state], {\n","                self._states:             states,\n","                self._taken_actions:      actions,\n","                self._discounted_rewards: rewards,\n","                self._learning_rate:      learning_rate,\n","                self._rnn_state_in: curr_rnn_state\n","            })\n","            curr_rnn_state = s\n","\n","        self._clean_up()\n","\n","    def _minibatches(self, samples, batch_size):\n","        for i in range(0, len(samples), batch_size):\n","            yield samples[i:i + batch_size]\n","\n","    def _gen_learning_rate(self, iteration, l_max, l_min, N_max):\n","        if iteration > N_max:\n","            return l_min\n","        alpha = 2 * l_max\n","        beta = np.log((alpha / l_min - 1)) / N_max\n","        return alpha / (1 + np.exp(beta * iteration))\n","\n","    def _clean_up(self):\n","        self._state_buffer  = []\n","        self._reward_buffer = []\n","        self._action_buffer = []\n","        self._curr_rnn_state = (np.zeros([1, self._rnn_hidden]), np.zeros([1, self._rnn_hidden]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OdgT5Talnbng","colab_type":"code","outputId":"5d7ab95c-720a-4cc3-bbf5-03fc073c1b80","executionInfo":{"status":"ok","timestamp":1591496218139,"user_tz":240,"elapsed":407569,"user":{"displayName":"ricardo carrillo","photoUrl":"","userId":"05727590381692514706"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import gym\n","\n","#from rl.reinforce.agent import *\n","\n","# MountainCarContinuous-v0 defines \"solving\" as getting an average reward of 90.0 over 100 consecutive trials.\n","env = gym.envs.make(\"MountainCarContinuous-v0\")\n","\n","state_dim = env.observation_space.shape[0]\n","\n","MAX_EPISODES = 200\n","MAX_STEPS = 1000\n","scores=[]\n","# agent = MLPStochasticPolicyAgent(env, num_input=state_dim, init_learning_rate=5e-5, min_learning_rate=1e-10,\n","#                                  learning_rate_N_max=2000, shuffle=True, batch_size=24, sigma=None)\n","\n","# agent = TFRecurrentStochasticPolicyAgent(env, num_input=1, init_learning_rate=5e-5, min_learning_rate=1e-10,\n","#                                          learning_rate_N_max=2000, shuffle=True, batch_size=24, sigma=None)\n","\n","agent = TFRecurrentStochasticPolicyAgent2(env, num_input=1, init_learning_rate=5e-5, min_learning_rate=1e-10,\n","                                         learning_rate_N_max=2000, shuffle=True, batch_size=1)\n","\n","# render an animation of each step in an episode\n","render = False\n","\n","if __name__ == \"__main__\":\n","\n","    for episode_counter in range(MAX_EPISODES):\n","        state = env.reset()\n","        total_rewards = 0\n","        sigmas = []\n","\n","        done = False\n","        for step_counter in range(MAX_STEPS):\n","            if render:\n","                env.render()\n","            action, sigma = agent.sample_action(state)\n","            next_state, reward, done, _ = env.step(action)\n","\n","            total_rewards += reward\n","            sigmas.append(sigma)\n","            agent.store_rollout(state, action, reward)\n","\n","            state = next_state\n","            if done:\n","                break\n","\n","        agent.update_model(episode_counter)\n","\n","        print(\"{},{:.2f}\".format(episode_counter, total_rewards))\n","        scores.append(total_rewards)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-1-b575e89509ad>:337: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-1-b575e89509ad>:340: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","0,-50.68\n","1,-51.63\n","2,-50.01\n","3,-44.33\n","4,-36.99\n","5,-32.63\n","6,-26.02\n","7,-22.25\n","8,-21.05\n","9,-18.52\n","10,-18.72\n","11,-17.09\n","12,-16.30\n","13,-16.35\n","14,-14.52\n","15,-14.86\n","16,-13.80\n","17,-12.85\n","18,-12.71\n","19,-12.44\n","20,-12.76\n","21,-12.40\n","22,-12.37\n","23,-12.75\n","24,-11.91\n","25,-11.80\n","26,-11.61\n","27,-10.16\n","28,-8.34\n","29,-8.45\n","30,-8.27\n","31,-7.74\n","32,-7.94\n","33,-7.81\n","34,-7.28\n","35,-7.17\n","36,-7.60\n","37,-8.25\n","38,-7.51\n","39,-7.36\n","40,-7.42\n","41,-7.20\n","42,-6.96\n","43,-7.04\n","44,-6.69\n","45,-7.12\n","46,-7.43\n","47,-7.73\n","48,-7.60\n","49,-7.15\n","50,-6.40\n","51,-6.30\n","52,-6.43\n","53,-6.02\n","54,-6.39\n","55,-6.53\n","56,-5.65\n","57,-5.79\n","58,-5.76\n","59,-5.77\n","60,-5.41\n","61,-4.97\n","62,-5.05\n","63,-5.01\n","64,-5.10\n","65,-4.79\n","66,-4.67\n","67,-4.49\n","68,-4.55\n","69,-4.55\n","70,-4.44\n","71,-4.92\n","72,-4.63\n","73,-4.97\n","74,-4.76\n","75,-4.94\n","76,-4.65\n","77,-4.66\n","78,-4.67\n","79,-4.72\n","80,-4.61\n","81,-4.79\n","82,-4.88\n","83,-4.56\n","84,-4.74\n","85,-4.87\n","86,-4.46\n","87,-4.56\n","88,-4.38\n","89,-4.22\n","90,-4.58\n","91,-4.32\n","92,-4.45\n","93,-4.63\n","94,-4.63\n","95,-4.36\n","96,-4.51\n","97,-4.49\n","98,-4.83\n","99,-4.95\n","100,-4.94\n","101,-4.77\n","102,-4.36\n","103,-4.65\n","104,-4.64\n","105,-4.33\n","106,-4.35\n","107,-4.43\n","108,-4.04\n","109,-4.19\n","110,-4.19\n","111,-4.14\n","112,-3.98\n","113,-3.69\n","114,-3.69\n","115,-3.48\n","116,-3.56\n","117,-3.41\n","118,-3.60\n","119,-3.34\n","120,-3.17\n","121,-3.08\n","122,-3.06\n","123,-3.12\n","124,-3.21\n","125,-3.12\n","126,-3.25\n","127,-2.95\n","128,-2.99\n","129,-3.18\n","130,-3.16\n","131,-3.03\n","132,-3.29\n","133,-3.03\n","134,-3.12\n","135,-2.80\n","136,-2.66\n","137,-2.63\n","138,-2.65\n","139,-2.60\n","140,-2.71\n","141,-2.72\n","142,-2.75\n","143,-2.54\n","144,-2.61\n","145,-2.48\n","146,-2.35\n","147,-2.36\n","148,-2.27\n","149,-2.18\n","150,-2.12\n","151,-2.13\n","152,-1.89\n","153,-1.86\n","154,-1.75\n","155,-1.80\n","156,-1.87\n","157,-1.81\n","158,-1.68\n","159,-1.83\n","160,-1.77\n","161,-1.80\n","162,-1.79\n","163,-1.67\n","164,-1.73\n","165,-1.73\n","166,-1.73\n","167,-1.80\n","168,-1.70\n","169,-1.61\n","170,-1.67\n","171,-1.80\n","172,-1.60\n","173,-1.66\n","174,-1.68\n","175,-1.70\n","176,-1.69\n","177,-1.68\n","178,-1.79\n","179,-1.58\n","180,-1.72\n","181,-1.74\n","182,-1.66\n","183,-1.62\n","184,-1.74\n","185,-1.70\n","186,-1.73\n","187,-1.67\n","188,-1.71\n","189,-1.73\n","190,-1.71\n","191,-1.68\n","192,-1.65\n","193,-1.71\n","194,-1.77\n","195,-1.61\n","196,-1.75\n","197,-1.68\n","198,-1.70\n","199,-1.76\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7gbEIiYQkSXn","colab_type":"code","outputId":"5b8baf09-a3b7-41a9-a91d-b0e99e1e33b9","executionInfo":{"status":"ok","timestamp":1591496256842,"user_tz":240,"elapsed":1706,"user":{"displayName":"ricardo carrillo","photoUrl":"","userId":"05727590381692514706"}},"colab":{"base_uri":"https://localhost:8080/","height":317}},"source":["import matplotlib\n","import matplotlib.pyplot as plt\n","# %matplotlib inline\n","print(scores)\n","plt.plot([i+1 for i in range(0,200,1)], scores[:])\n","plt.xlabel('Episode no.')\n","plt.ylabel('Score')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[-50.68301118686304, -51.63394594881651, -50.00978173593548, -44.32965827319212, -36.99019050557729, -32.63375125740115, -26.017612024269443, -22.246371325165217, -21.045545038374293, -18.523171625775333, -18.719867205879474, -17.088631066557184, -16.297340781826176, -16.347622987571878, -14.523841745345893, -14.855108585686477, -13.803778017554524, -12.854314660438567, -12.709491364670605, -12.44055828352075, -12.756998619786208, -12.401647788100261, -12.372760386226052, -12.75231386714739, -11.913680229567682, -11.804424060193002, -11.605552615210737, -10.156152491521873, -8.339242450254726, -8.447985159145999, -8.265669571171943, -7.736084797796057, -7.940080687016453, -7.80993892495554, -7.276538891841811, -7.173084395447599, -7.603752466193373, -8.249287722035284, -7.5073554781423715, -7.364164496759068, -7.41697720074124, -7.201639089741607, -6.9578594660222315, -7.038949524619975, -6.687848193057497, -7.11938130307855, -7.43225788001443, -7.727950180947147, -7.604169455925545, -7.15134896465487, -6.395848584181269, -6.302352277805767, -6.432261878528096, -6.022849842551401, -6.38786909378893, -6.533765955364311, -5.647983366251118, -5.790879560174959, -5.758422436937132, -5.769943359258537, -5.412792407391889, -4.966389478723735, -5.049006343131528, -5.01175724968218, -5.095653922313582, -4.790361263459494, -4.674417918831888, -4.494963738023959, -4.5510216115053845, -4.550515433598179, -4.44497280354482, -4.924242707333691, -4.629158853738656, -4.969276038549383, -4.758617750029032, -4.937707267046366, -4.6457434225028, -4.664556807418655, -4.674849655347977, -4.716087408907045, -4.6090964204369245, -4.787683873583007, -4.881984700931409, -4.55956410462494, -4.737405365657068, -4.867957696328617, -4.456105620512465, -4.558986498041711, -4.377667254983959, -4.219848965922543, -4.584355679961643, -4.322098295662909, -4.451880182999174, -4.625780929945867, -4.631645775643833, -4.360118018687774, -4.507743751973223, -4.488790707717028, -4.8343045899549635, -4.9500997306482954, -4.935932808588232, -4.769505003529957, -4.362914864428803, -4.650599392985331, -4.643983753211336, -4.330038105583753, -4.345176798964699, -4.430222785565183, -4.0372483588444075, -4.193139964345847, -4.188307432576432, -4.13573914429572, -3.980872721196904, -3.694358937454463, -3.688596417483011, -3.4779777242371654, -3.5570556783150487, -3.4060804362738435, -3.59818114489714, -3.3374870813222013, -3.16924476849009, -3.0845155798918356, -3.0569334302374798, -3.116957529352101, -3.2117952570100394, -3.11809265054358, -3.245986588472041, -2.946648282345453, -2.991821071535583, -3.177964414953749, -3.1560256670018427, -3.0319719746162623, -3.2862935097145534, -3.0251907774580995, -3.1213840995268956, -2.797887451017213, -2.6568088412711766, -2.6348135765315917, -2.6452024378002097, -2.595744654773594, -2.7091400551891596, -2.717182917691874, -2.747755899242257, -2.5433898796370804, -2.6079408622763345, -2.4777862983047596, -2.3525695212551136, -2.3567530569309723, -2.2698598758850497, -2.179530775097556, -2.1229473566550654, -2.1298685362008203, -1.8896399310095928, -1.8584452607476023, -1.7505427704234109, -1.801788438592063, -1.8657723232015946, -1.8053800197127985, -1.6819477142225603, -1.8318320537803696, -1.7661154539930548, -1.8048830165322458, -1.7949386791385635, -1.665923460658069, -1.7342225115554122, -1.7305647001332576, -1.7335211676554525, -1.7959530273866724, -1.6978172035598875, -1.6146264119197435, -1.6737701760835657, -1.8034904719825422, -1.6022022083558038, -1.659352236594176, -1.6751608986206805, -1.7012307289933521, -1.6853675685091931, -1.6751174047854358, -1.78554845939983, -1.5829405164946009, -1.7188417194754577, -1.7358085511663575, -1.6567196924913234, -1.615917786173627, -1.7368016668164845, -1.6950135839412386, -1.7316077789813238, -1.671517249986548, -1.7137876890099526, -1.7295645380926374, -1.7093725446705004, -1.6839748622463138, -1.6462903298312481, -1.708793611514767, -1.7730852640166195, -1.6090583678491583, -1.7464950760262419, -1.6794329439921432, -1.6970689313933969, -1.7639801826410293]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}